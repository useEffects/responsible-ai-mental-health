\section{Literature Review}
Park et al. \cite{park2024building} developed 100 benchmark questions and 5 guideline questions for safety assessment. The framework aims to promote the responsible integration and effectiveness of healthcare chatbots, thereby enhancing their safety and building trust among users and professionals. Additionally, LLM-based evaluation tools were evaluated to streamline the evaluation process, presenting the effectiveness and limitations of these tools for mental health chatbots. The Agent and Embedding methods demonstrated the most accurate alignments with human assessments. Guo et al. \cite{guo2024large} demonstrate that LLMs are highly effective in analyzing textual data to assess mental states and identify suicidal ideation. They show great potential for providing mental health interventions with improved prognoses, with the majority being recognized by psychologists for their appropriateness and accuracy. Wang et al. \cite{wang2024towards} focus on a client-centered assessment of LLM therapists with the involvement of simulated clients, a standard approach in clinical medical education. It is observed that LLM therapists achieve comparably lower scores in terms of self-reported feelings compared to human therapists. LLM therapists excessively focus on emotions, as evidenced by the higher frequency of emotion-related words. Hongbin Na \cite{na2024cbt} in their study presented a pioneering approach in the realm of psychological health support, bridging the gap between LLM and Cognitive Behavior Therapy (CBT). By introducing a CBT-specific prompt and crafting the tailored CBT QA dataset for the Chinese mental health landscape and fine-tune a large language model, CBT-LLM was established. Studies by Steenstra et al. \cite{steenstra2024virtual}  provide valuable insights into the potential use of LLMs in generating human-like counselor dialog, which can serve as the backbone for virtual agents providing health counseling. Case study were conducted to evaluate the capabilities of LLMs in providing alcohol use counseling via Motivational Interviewing. Based on the structure of the prompt we designed, other researchers and practitioners can modify it to align an LLM with their desired task domain, such as smoking cessation, nutrition and exercise counseling and chronic disease management. Studies by Song et al. \cite{song2024typing} found that participants did find value in using LLM chatbots for mental health, and that this value often aligned with principles around what makes support effective. However, the general purpose nature of how most publicly available and commonly used LLMs are trained led to broad and non-specific answers that could be culturally mismatched with the needs of a participant. Li \& Kim \cite{lisocial} built Social Robot for the Depressed and Lonely that analyzes the user's current emotional state and takes appropriate interactions based on it.