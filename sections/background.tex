\section{Background}
Loneliness is increasingly recognized as the next critical public health issue. A plausible reason for this concern may be related to emerging societal trends affecting the way we relate, communicate, and function in our social environment. If we compare two individuals of the same age - one today and another one a generation ago - we would find that the one today is more likely to feel lonely. This is based on the idea that there have been societal changes - such as the rise of living alone - that make newer generations more likely to feel lonely \cite{owid-loneliness-epidemic}. In England, the Office for National Statistics conducts the Community Life Survey, in which they ask people how often they feel lonely. According to this data, those aged 16 to 24 are the group most likely to report feeling lonely, with 10\% feeling lonely "often or always". In contrast, those aged 65 years and older are the group least likely to report feeling lonely, with 3\% feeling lonely "often or always". \\
Despite the potential benefits, the integration of generative AI in virtual companions raises significant concerns that need to be addressed to ensure its responsible deployment. AI models like ChatGPT are trained on vast amounts of text data from the internet. If this data contains biases (e.g., cultural, racial, gender-based), these biases can be learned and perpetuated by the model. Experiments show that GPT-3 has higher levels of violent prejudice against Muslims than other religious groups \cite{abid2021persistent}. TRUSTGPT \cite{huang2023trustgpt} argues that if a model is biased against one demographic group, it will generate more toxic content for that group than for other groups. For companies using LLMs, secure messaging involves two key areas: data transfer and backend security. Data transfer ensures safe transmission of messages to and from the chatbot. Backend security focuses on how data is processed, stored, and shared on the server \cite{hasal2021chatbots}. IBM partnered with Oxford Economics in 2021 reported only 40\% of those surveyed, trust companies to be responsible and ethical in their use of new technologies.  PsychoBench \cite{huang2023humanity}, a comprehensive framework for evaluating LLMs' psychological representations reported LLMs consistently demonstrate high scores on \textit{Lying} subscale of the EPQ-R. Current laws are inadequate for addressing the subtle reinforcement of stereotypes by LLMs, particularly against marginalized groups, and the exacerbation of socio-economic disparities resulting from unequal access to the benefits of LLMs. Defamation claims often fail without clear evidence that the output is false and targets specific individuals. Product liability claims are typically concerned with physical injury, which is less likely in LLM use; even if such injuries occur, proving that LLMs were the sole cause of the harm can be challenging due to the complex interactions between LLMs and human users. Additionally, criminal law does not currently provide remedies for virtual sexual abuse facilitated by LLMs, despite the severe nature of such harms \cite{cheong2022envisioning}. The idea that AI friends are "caring, kind, and helpful" when compared to real friends known as warmth, also have an effect on possible addictive behaviors. Feelings of being treated kindly and friendly can result in a passionate continue use of the AI friendship app for the purpose of enhancing well-being, which can influence the level of addiction toward using the app \cite{marriott2024one}. We also unsure what happens when there is a widespread adoption of virtual companions rather than using it only during loneliness.