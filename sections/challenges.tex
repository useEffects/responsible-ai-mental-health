\section{Challenges}
Using LLM in AI Companion app brings the challenges faced by LLMs into the focus as well \cite{sarker2024llm}, briefly the four major ethical issues must be addressed \cite{gerke2020ethical}: (1) informed consent to use data: users must clearly understand and agree to how their sensitive emotional data, their user data, chat messages are collected, stored, and used. (2) safety and transparency: the AI companion app should explain its decisions and behaviors clearly, ensuring users understand why it's responding in a certain way. This transparency helps build trust and ensures users aren't confused or misled by its inferences. (3) algorithmic fairness and biases: The AI must treat all users equitably, regardless of background, avoiding biased responses reinforce social isolation for certain groups. and (4) data privacy: The AI must protect users' personal and emotional data to prevent breaches, which could expose individuals' vulnerabilities. While hardening LLMs for safety can help, models remain vulnerable to adversarial inputs, as demonstrated by the spread of "jailbreaks" for ChatGPT since its initial release \cite{andriushchenko2024jailbreaking}. Another jailbreak approach known as mosaic prompting \cite{glukhov2023llm}, demonstrates that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. Unintended consequences may change the clinical profession \cite{stade2024large}. Clinical LLMs could shift mental health services, with non-professionals handling more cases under clinician supervision. This may reduce direct patient care, increase burnout, and alter consumer expectations, conflicting with traditional psychotherapy practices. Research on safe caseloads and updated guidelines is needed.